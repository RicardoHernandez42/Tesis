{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graphic-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation, Concatenate, BatchNormalization\n",
    "from tensorflow.python.keras.layers import Input, Conv2D, Dense, concatenate, MaxPooling2D, AveragePooling2D, Dense\n",
    "from tensorflow.python.keras.layers.advanced_activations import PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenin dominguez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jewish-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electrical-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entrenamiento = '/Volumes/ADATA/EntrenamientoIm/'\n",
    "data_validacion = '/Volumes/ADATA/Validaci√≥nIm/'\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "objective-mailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richo/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:179: UserWarning: Using \".tiff\" files with multiple bands will cause distortion. Please verify your output.\n",
      "  warnings.warn('Using \".tiff\" files with multiple bands '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7335 images belonging to 2 classes.\n",
      "Found 2443 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "entrenamiento_datagen = ImageDataGenerator()\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento,\n",
    "    target_size=(5, 1700),\n",
    "    batch_size= 33,\n",
    "    class_mode = 'binary')\n",
    "\n",
    "validacion_generador = test_datagen.flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(5, 1700),\n",
    "    batch_size= 33,\n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    LCI = Input(shape=(5,1700,3))\n",
    "#BloqueA  \n",
    "    P1  = AveragePooling2D((5,1), strides=2, padding='same')(LCI)\n",
    "    TC1 = Conv2D(32, (11,1), padding='same', activation='tanh')(P1)\n",
    "    TC1 = BatchNormalization()(TC1)\n",
    "    TC2 = Conv2D(32, (21,1), padding='same', activation='tanh')(P1)\n",
    "    TC2 = BatchNormalization()(TC2)\n",
    "    TC3 = Conv2D(32, (41,1), padding='same', activation='tanh')(P1)\n",
    "    TC3 = BatchNormalization()(TC3)\n",
    "    P2  = AveragePooling2D((11,1), strides=2, padding='same')(LCI)\n",
    "    TC4 = Conv2D(32, (11,1), padding='same', activation='tanh')(P2)\n",
    "    TC4 = BatchNormalization()(TC4)\n",
    "    TC5 = Conv2D(32, (21,1), padding='same', activation='tanh')(P2)\n",
    "    TC5 = BatchNormalization()(TC5)\n",
    "    TC6 = Conv2D(32, (41,1), padding='same', activation='tanh')(P2)\n",
    "    TC6 = BatchNormalization()(TC6)\n",
    "    C1  = concatenate([TC1,TC2,TC3,TC4,TC5,TC6])\n",
    "    MC1 = Conv2D(64, (1, 5), padding='same')(C1)\n",
    "    MC1 = PReLU()(MC1)\n",
    "    MC1 = BatchNormalization()(MC1) \n",
    "    TC7_1 = Conv2D(64, (11,1), padding='same')(C1)\n",
    "    TC7_1 = PReLU()(TC7_1)\n",
    "    TC7_1 = BatchNormalization()(TC7_1)\n",
    "    TC8_1 = Conv2D(64, (21,1), padding='same')(C1)\n",
    "    TC8_1 = PReLU()(TC8_1)\n",
    "    TC8_1 = BatchNormalization()(TC8_1)\n",
    "    TC7_2 = Conv2D(64, (11,1), padding='same')(MC1)\n",
    "    TC7_2 = PReLU()(TC7_2)\n",
    "    TC7_2 = BatchNormalization()(TC7_2)\n",
    "    TC8_2 = Conv2D(64, (21,1), padding='same')(MC1)\n",
    "    TC8_2 = PReLU()(TC8_2)\n",
    "    TC8_2 = BatchNormalization()(TC8_2)\n",
    "    C2  = concatenate([TC7_1,TC8_1])\n",
    "    C3  = concatenate([TC7_2,TC8_2])\n",
    "#BloqueB\n",
    "    P3  = MaxPooling2D((3,1), strides=2, padding='same')(C2)\n",
    "    P4  = MaxPooling2D((3,1), strides=2, padding='same')(C3)\n",
    "    P5  = AveragePooling2D((21,1), strides=4, padding='same')(LCI)\n",
    "    TC9 = Conv2D(32, (5,1), padding='same', activation='tanh')(P5)\n",
    "    TC9 = BatchNormalization()(TC9)\n",
    "    TC10 = Conv2D(32, (11,1), padding='same', activation='tanh')(P5)\n",
    "    TC10 = BatchNormalization()(TC10)\n",
    "    TC11 = Conv2D(32, (21,1), padding='same', activation='tanh')(P5) \n",
    "    TC11 = BatchNormalization()(TC11)\n",
    "    C4  = concatenate([TC9,TC10,TC11])\n",
    "    TC12 = Conv2D(64, (11,1), padding='same')(C4)\n",
    "    TC12 = PReLU()(TC12)\n",
    "    TC12 = BatchNormalization()(TC12)\n",
    "    TC13 = Conv2D(64, (21,1), padding='same')(C4)\n",
    "    TC13 = PReLU()(TC13)\n",
    "    TC13 = BatchNormalization()(TC13)\n",
    "    C5  = concatenate([TC12,TC13])\n",
    "    MC2 = Conv2D(128, (1, 5), padding='same')(C5)\n",
    "    MC2 = PReLU()(MC2)\n",
    "    MC2 = BatchNormalization()(MC2)\n",
    "    C6  = concatenate([P3,MC2])\n",
    "    C7  = concatenate([P4,C5])\n",
    "    TC14_1 = Conv2D(256, (11,1), padding='same')(C6)\n",
    "    TC14_1 = PReLU()(TC14_1)\n",
    "    TC14_1 = BatchNormalization()(TC14_1)\n",
    "    TC14_2 = Conv2D(256, (11,1), padding='same')(C7)\n",
    "    TC14_2 = PReLU()(TC14_2)\n",
    "    TC14_2 = BatchNormalization()(TC14_2)\n",
    "#BloqueC\n",
    "    P6 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_1)\n",
    "    P7 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_2)\n",
    "    TC15_1 = Conv2D(512, (11,1), padding='same')(P6)\n",
    "    TC15_1 = PReLU()(TC15_1)\n",
    "    TC15_1 = BatchNormalization()(TC15_1)\n",
    "    TC15_2 = Conv2D(512, (11,1), padding='same')(P7)\n",
    "    TC15_2 = PReLU()(TC15_2)\n",
    "    TC15_2 = BatchNormalization()(TC15_2)\n",
    "#BloqueD\n",
    "    P8 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_1)\n",
    "    P9 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_2)\n",
    "    P10 = AveragePooling2D((41,1), strides=16, padding='same')(LCI)\n",
    "    TC16 = Conv2D(32, (5,1), padding='same', activation='tanh')(P10)\n",
    "    TC16 = BatchNormalization()(TC16)\n",
    "    TC17 = Conv2D(32, (11,1), padding='same', activation='tanh')(P10)\n",
    "    TC17 = BatchNormalization()(TC17)\n",
    "    TC18 = Conv2D(32, (21,1), padding='same', activation='tanh')(P10)\n",
    "    TC18 = BatchNormalization()(TC18)\n",
    "    C8  = concatenate([TC16,TC17,TC18])\n",
    "    TC19 = Conv2D(64, (11,1), padding='same')(C8)\n",
    "    TC19 = PReLU()(TC19)\n",
    "    TC19 = BatchNormalization()(TC19)\n",
    "    MC3 = Conv2D(64, (1, 5), padding='same')(TC19)\n",
    "    MC3 = PReLU()(MC3)\n",
    "    MC3 = BatchNormalization()(MC3)\n",
    "    C9  = concatenate([P8,MC3])\n",
    "    C10  = concatenate([P9,TC19])\n",
    "    TC20_1 = Conv2D(128, (11,1), padding='same')(C9)\n",
    "    TC20_1 = PReLU()(TC20_1)\n",
    "    TC20_1 = BatchNormalization()(TC20_1)\n",
    "    TC20_2 = Conv2D(128, (11,1), padding='same')(C10)\n",
    "    TC20_2 = PReLU()(TC20_2)\n",
    "    TC20_2 = BatchNormalization()(TC20_2)\n",
    "#BloqueE\n",
    "    P11 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_1)\n",
    "    P12 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_2)\n",
    "    P13 = AveragePooling2D((61,1), strides=32, padding='same')(LCI)\n",
    "    TC21 = Conv2D(256, (5,1), padding='same', activation='tanh')(P13)\n",
    "    TC21 = BatchNormalization()(TC21)\n",
    "    TC22 = Conv2D(256, (11,1), padding='same', activation='tanh')(P13)\n",
    "    TC22 = BatchNormalization()(TC22)\n",
    "    TC23 = Conv2D(256, (21,1), padding='same', activation='tanh')(P13)\n",
    "    TC23 = BatchNormalization()(TC23)\n",
    "    C11  = concatenate([TC21,TC22,TC23])   \n",
    "    TC24 = Conv2D(512, (11,1), padding='same')(C11)  \n",
    "    TC24 = PReLU()(TC24)\n",
    "    TC24 = BatchNormalization()(TC24)\n",
    "    C12  = concatenate([P12,TC24])       \n",
    "    TC25 = Conv2D(512, (11,1), padding='same')(C12)\n",
    "    TC25 = PReLU()(TC25)\n",
    "    TC25 = BatchNormalization()(TC25)\n",
    "    TC26 = Conv2D(512, (21,1), padding='same')(C12)\n",
    "    TC26 = PReLU()(TC26)\n",
    "    TC26 = BatchNormalization()(TC26)\n",
    "    C13  = concatenate([TC25,TC26])  \n",
    "    MC4 = Conv2D(1024, (1, 5), padding='same')(C13)\n",
    "    MC4 = PReLU()(MC4)\n",
    "    MC4 = BatchNormalization()(MC4)\n",
    "    C14  = concatenate([P11,MC4]) \n",
    "    TC27 = Conv2D(2048, (5,1), padding='same')(C14)\n",
    "    TC27 = PReLU()(TC27)\n",
    "    TC27 = BatchNormalization()(TC27)\n",
    "    TC28 = Conv2D(2048, (11,1), padding='same')(C14)\n",
    "    TC28 = PReLU()(TC28)\n",
    "    TC28 = BatchNormalization()(TC28)\n",
    "    TC29 = Conv2D(2048, (21,1), padding='same')(C14)\n",
    "    TC29 = PReLU()(TC29)\n",
    "    TC29 = BatchNormalization()(TC29)\n",
    "    TC30 = Conv2D(2048, (41,1), padding='same')(C14) \n",
    "    TC30 = PReLU()(TC30)\n",
    "    TC30 = BatchNormalization()(TC30)\n",
    "    C15  = concatenate([TC27,TC28,TC29,TC30]) \n",
    "#FullyConectedLayers\n",
    "    FC1 = Flatten()(C15)\n",
    "    merged_1 = Dense(8192, activation='relu')(FC1)\n",
    "    merged_2 = Dropout(0.5)(merged_1)\n",
    "    FC2 = Flatten()(merged_2)\n",
    "    merged_3 = Dense(8192, activation='relu')(FC2) \n",
    "    merged_4 = Dropout(0.5)(merged3)\n",
    "    SM = Dense(2, activation='softmax')(merged_4)\n",
    "#Entrenamiento\n",
    "    model = Model(LCI, SM)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit_generator(entrenamiento_generador, \n",
    "                             steps_per_epoch = int(7335/float(batch_size)),\n",
    "                             epochs = 2,\n",
    "                             validation_data = validacion_generador,\n",
    "                             validation_steps = int(2443/float(batch_size)))\n",
    "    return model\n",
    "print(cnn(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    LCI = Input(shape=(5,1700,3))\n",
    "#BloqueA  \n",
    "    P1  = AveragePooling2D((5,1), strides=2, padding='same')(LCI)\n",
    "    TC1 = Conv2D(32, (11,1), padding='same', activation='tanh')(P1)\n",
    "    TC1 = BatchNormalization()(TC1)\n",
    "    TC2 = Conv2D(32, (21,1), padding='same', activation='tanh')(P1)\n",
    "    TC2 = BatchNormalization()(TC2)\n",
    "    TC3 = Conv2D(32, (41,1), padding='same', activation='tanh')(P1)\n",
    "    TC3 = BatchNormalization()(TC3)\n",
    "    P2  = AveragePooling2D((11,1), strides=2, padding='same')(LCI)\n",
    "    TC4 = Conv2D(32, (11,1), padding='same', activation='tanh')(P2)\n",
    "    TC4 = BatchNormalization()(TC4)\n",
    "    TC5 = Conv2D(32, (21,1), padding='same', activation='tanh')(P2)\n",
    "    TC5 = BatchNormalization()(TC5)\n",
    "    TC6 = Conv2D(32, (41,1), padding='same', activation='tanh')(P2)\n",
    "    TC6 = BatchNormalization()(TC6)\n",
    "    C1  = concatenate([TC1,TC2,TC3,TC4,TC5,TC6])\n",
    "    MC1 = Conv2D(64, (1, 5), padding='same')(C1)\n",
    "    MC1 = PReLU()(MC1)\n",
    "    MC1 = BatchNormalization()(MC1) \n",
    "    TC7_1 = Conv2D(64, (11,1), padding='same')(C1)\n",
    "    TC7_1 = PReLU()(TC7_1)\n",
    "    TC7_1 = BatchNormalization()(TC7_1)\n",
    "    TC8_1 = Conv2D(64, (21,1), padding='same')(C1)\n",
    "    TC8_1 = PReLU()(TC8_1)\n",
    "    TC8_1 = BatchNormalization()(TC8_1)\n",
    "    TC7_2 = Conv2D(64, (11,1), padding='same')(MC1)\n",
    "    TC7_2 = PReLU()(TC7_2)\n",
    "    TC7_2 = BatchNormalization()(TC7_2)\n",
    "    TC8_2 = Conv2D(64, (21,1), padding='same')(MC1)\n",
    "    TC8_2 = PReLU()(TC8_2)\n",
    "    TC8_2 = BatchNormalization()(TC8_2)\n",
    "    C2  = concatenate([TC7_1,TC8_1])\n",
    "    C3  = concatenate([TC7_2,TC8_2])\n",
    "#BloqueB\n",
    "    P3  = MaxPooling2D((3,1), strides=2, padding='same')(C2)\n",
    "    P4  = MaxPooling2D((3,1), strides=2, padding='same')(C3)\n",
    "    P5  = AveragePooling2D((21,1), strides=4, padding='same')(LCI)\n",
    "    TC9 = Conv2D(32, (5,1), padding='same', activation='tanh')(P5)\n",
    "    TC9 = BatchNormalization()(TC9)\n",
    "    TC10 = Conv2D(32, (11,1), padding='same', activation='tanh')(P5)\n",
    "    TC10 = BatchNormalization()(TC10)\n",
    "    TC11 = Conv2D(32, (21,1), padding='same', activation='tanh')(P5) \n",
    "    TC11 = BatchNormalization()(TC11)\n",
    "    C4  = concatenate([TC9,TC10,TC11])\n",
    "    TC12 = Conv2D(64, (11,1), padding='same')(C4)\n",
    "    TC12 = PReLU()(TC12)\n",
    "    TC12 = BatchNormalization()(TC12)\n",
    "    TC13 = Conv2D(64, (21,1), padding='same')(C4)\n",
    "    TC13 = PReLU()(TC13)\n",
    "    TC13 = BatchNormalization()(TC13)\n",
    "    C5  = concatenate([TC12,TC13])\n",
    "    MC2 = Conv2D(128, (1, 5), padding='same')(C5)\n",
    "    MC2 = PReLU()(MC2)\n",
    "    MC2 = BatchNormalization()(MC2)\n",
    "    C6  = concatenate([P3,MC2])\n",
    "    C7  = concatenate([P4,C5])\n",
    "    TC14_1 = Conv2D(256, (11,1), padding='same')(C6)\n",
    "    TC14_1 = PReLU()(TC14_1)\n",
    "    TC14_1 = BatchNormalization()(TC14_1)\n",
    "    TC14_2 = Conv2D(256, (11,1), padding='same')(C7)\n",
    "    TC14_2 = PReLU()(TC14_2)\n",
    "    TC14_2 = BatchNormalization()(TC14_2)\n",
    "#BloqueC\n",
    "    P6 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_1)\n",
    "    P7 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_2)\n",
    "    TC15_1 = Conv2D(512, (11,1), padding='same')(P6)\n",
    "    TC15_1 = PReLU()(TC15_1)\n",
    "    TC15_1 = BatchNormalization()(TC15_1)\n",
    "    TC15_2 = Conv2D(512, (11,1), padding='same')(P7)\n",
    "    TC15_2 = PReLU()(TC15_2)\n",
    "    TC15_2 = BatchNormalization()(TC15_2)\n",
    "#BloqueD\n",
    "    P8 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_1)\n",
    "    P9 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_2)\n",
    "    P10 = AveragePooling2D((41,1), strides=16, padding='same')(LCI)\n",
    "    TC16 = Conv2D(32, (5,1), padding='same', activation='tanh')(P10)\n",
    "    TC16 = BatchNormalization()(TC16)\n",
    "    TC17 = Conv2D(32, (11,1), padding='same', activation='tanh')(P10)\n",
    "    TC17 = BatchNormalization()(TC17)\n",
    "    TC18 = Conv2D(32, (21,1), padding='same', activation='tanh')(P10)\n",
    "    TC18 = BatchNormalization()(TC18)\n",
    "    C8  = concatenate([TC16,TC17,TC18])\n",
    "    TC19 = Conv2D(64, (11,1), padding='same')(C8)\n",
    "    TC19 = PReLU()(TC19)\n",
    "    TC19 = BatchNormalization()(TC19)\n",
    "    MC3 = Conv2D(64, (1, 5), padding='same')(TC19)\n",
    "    MC3 = PReLU()(MC3)\n",
    "    MC3 = BatchNormalization()(MC3)\n",
    "    C9  = concatenate([P8,MC3])\n",
    "    C10  = concatenate([P9,TC19])\n",
    "    TC20_1 = Conv2D(128, (11,1), padding='same')(C9)\n",
    "    TC20_1 = PReLU()(TC20_1)\n",
    "    TC20_1 = BatchNormalization()(TC20_1)\n",
    "    TC20_2 = Conv2D(128, (11,1), padding='same')(C10)\n",
    "    TC20_2 = PReLU()(TC20_2)\n",
    "    TC20_2 = BatchNormalization()(TC20_2)\n",
    "#BloqueE\n",
    "    P11 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_1)\n",
    "    P12 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_2)\n",
    "    P13 = AveragePooling2D((61,1), strides=32, padding='same')(LCI)\n",
    "    TC21 = Conv2D(256, (5,1), padding='same', activation='tanh')(P13)\n",
    "    TC21 = BatchNormalization()(TC21)\n",
    "    TC22 = Conv2D(256, (11,1), padding='same', activation='tanh')(P13)\n",
    "    TC22 = BatchNormalization()(TC22)\n",
    "    TC23 = Conv2D(256, (21,1), padding='same', activation='tanh')(P13)\n",
    "    TC23 = BatchNormalization()(TC23)\n",
    "    C11  = concatenate([TC21,TC22,TC23])   \n",
    "    TC24 = Conv2D(512, (11,1), padding='same')(C11)  \n",
    "    TC24 = PReLU()(TC24)\n",
    "    TC24 = BatchNormalization()(TC24)\n",
    "    C12  = concatenate([P12,TC24])       \n",
    "    TC25 = Conv2D(512, (11,1), padding='same')(C12)\n",
    "    TC25 = PReLU()(TC25)\n",
    "    TC25 = BatchNormalization()(TC25)\n",
    "    TC26 = Conv2D(512, (21,1), padding='same')(C12)\n",
    "    TC26 = PReLU()(TC26)\n",
    "    TC26 = BatchNormalization()(TC26)\n",
    "    C13  = concatenate([TC25,TC26])  \n",
    "    MC4 = Conv2D(1024, (1, 5), padding='same')(C13)\n",
    "    MC4 = PReLU()(MC4)\n",
    "    MC4 = BatchNormalization()(MC4)\n",
    "    C14  = concatenate([P11,MC4]) \n",
    "    TC27 = Conv2D(2048, (5,1), padding='same')(C14)\n",
    "    TC27 = PReLU()(TC27)\n",
    "    TC27 = BatchNormalization()(TC27)\n",
    "    TC28 = Conv2D(2048, (11,1), padding='same')(C14)\n",
    "    TC28 = PReLU()(TC28)\n",
    "    TC28 = BatchNormalization()(TC28)\n",
    "    TC29 = Conv2D(2048, (21,1), padding='same')(C14)\n",
    "    TC29 = PReLU()(TC29)\n",
    "    TC29 = BatchNormalization()(TC29)\n",
    "    TC30 = Conv2D(2048, (41,1), padding='same')(C14) \n",
    "    TC30 = PReLU()(TC30)\n",
    "    TC30 = BatchNormalization()(TC30)\n",
    "    C15  = concatenate([TC27,TC28,TC29,TC30]) \n",
    "#FullyConectedLayers\n",
    "    FC1 = Flatten()(C15)\n",
    "    merged_1 = Dense(8192, activation='relu')(FC1)\n",
    "    merged_2 = Dropout(0.5)(merged_1)\n",
    "\n",
    "    SM = Dense(2, activation='softmax')(merged_2)\n",
    "#Entrenamiento\n",
    "    model = Model(LCI, SM)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit_generator(entrenamiento_generador, \n",
    "                             steps_per_epoch = int(7335/float(batch_size)),\n",
    "                             epochs = 2,\n",
    "                             validation_data = validacion_generador,\n",
    "                             validation_steps = int(2443/float(batch_size)))\n",
    "    return model\n",
    "print(cnn(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    LCI = Input(shape=(5,1700,3))\n",
    "#BloqueA  \n",
    "    P1  = AveragePooling2D((5,1), strides=2, padding='same')(LCI)\n",
    "    TC1 = Conv2D(32, (11,1), padding='same', activation='tanh')(P1)\n",
    "    TC1 = BatchNormalization()(TC1)\n",
    "    TC2 = Conv2D(32, (21,1), padding='same', activation='tanh')(P1)\n",
    "    TC2 = BatchNormalization()(TC2)\n",
    "    TC3 = Conv2D(32, (41,1), padding='same', activation='tanh')(P1)\n",
    "    TC3 = BatchNormalization()(TC3)\n",
    "    P2  = AveragePooling2D((11,1), strides=2, padding='same')(LCI)\n",
    "    TC4 = Conv2D(32, (11,1), padding='same', activation='tanh')(P2)\n",
    "    TC4 = BatchNormalization()(TC4)\n",
    "    TC5 = Conv2D(32, (21,1), padding='same', activation='tanh')(P2)\n",
    "    TC5 = BatchNormalization()(TC5)\n",
    "    TC6 = Conv2D(32, (41,1), padding='same', activation='tanh')(P2)\n",
    "    TC6 = BatchNormalization()(TC6)\n",
    "    C1  = concatenate([TC1,TC2,TC3,TC4,TC5,TC6])\n",
    "    MC1 = Conv2D(64, (1, 5), padding='same')(C1)\n",
    "    MC1 = PReLU()(MC1)\n",
    "    MC1 = BatchNormalization()(MC1) \n",
    "    TC7_1 = Conv2D(64, (11,1), padding='same')(C1)\n",
    "    TC7_1 = PReLU()(TC7_1)\n",
    "    TC7_1 = BatchNormalization()(TC7_1)\n",
    "    TC8_1 = Conv2D(64, (21,1), padding='same')(C1)\n",
    "    TC8_1 = PReLU()(TC8_1)\n",
    "    TC8_1 = BatchNormalization()(TC8_1)\n",
    "    TC7_2 = Conv2D(64, (11,1), padding='same')(MC1)\n",
    "    TC7_2 = PReLU()(TC7_2)\n",
    "    TC7_2 = BatchNormalization()(TC7_2)\n",
    "    TC8_2 = Conv2D(64, (21,1), padding='same')(MC1)\n",
    "    TC8_2 = PReLU()(TC8_2)\n",
    "    TC8_2 = BatchNormalization()(TC8_2)\n",
    "    C2  = concatenate([TC7_1,TC8_1])\n",
    "    C3  = concatenate([TC7_2,TC8_2])\n",
    "#BloqueB\n",
    "    P3  = MaxPooling2D((3,1), strides=2, padding='same')(C2)\n",
    "    P4  = MaxPooling2D((3,1), strides=2, padding='same')(C3)\n",
    "    P5  = AveragePooling2D((21,1), strides=4, padding='same')(LCI)\n",
    "    TC9 = Conv2D(32, (5,1), padding='same', activation='tanh')(P5)\n",
    "    TC9 = BatchNormalization()(TC9)\n",
    "    TC10 = Conv2D(32, (11,1), padding='same', activation='tanh')(P5)\n",
    "    TC10 = BatchNormalization()(TC10)\n",
    "    TC11 = Conv2D(32, (21,1), padding='same', activation='tanh')(P5) \n",
    "    TC11 = BatchNormalization()(TC11)\n",
    "    C4  = concatenate([TC9,TC10,TC11])\n",
    "    TC12 = Conv2D(64, (11,1), padding='same')(C4)\n",
    "    TC12 = PReLU()(TC12)\n",
    "    TC12 = BatchNormalization()(TC12)\n",
    "    TC13 = Conv2D(64, (21,1), padding='same')(C4)\n",
    "    TC13 = PReLU()(TC13)\n",
    "    TC13 = BatchNormalization()(TC13)\n",
    "    C5  = concatenate([TC12,TC13])\n",
    "    MC2 = Conv2D(128, (1, 5), padding='same')(C5)\n",
    "    MC2 = PReLU()(MC2)\n",
    "    MC2 = BatchNormalization()(MC2)\n",
    "    C6  = concatenate([P3,MC2])\n",
    "    C7  = concatenate([P4,C5])\n",
    "    TC14_1 = Conv2D(256, (11,1), padding='same')(C6)\n",
    "    TC14_1 = PReLU()(TC14_1)\n",
    "    TC14_1 = BatchNormalization()(TC14_1)\n",
    "    TC14_2 = Conv2D(256, (11,1), padding='same')(C7)\n",
    "    TC14_2 = PReLU()(TC14_2)\n",
    "    TC14_2 = BatchNormalization()(TC14_2)\n",
    "#BloqueC\n",
    "    P6 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_1)\n",
    "    P7 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_2)\n",
    "    TC15_1 = Conv2D(512, (11,1), padding='same')(P6)\n",
    "    TC15_1 = PReLU()(TC15_1)\n",
    "    TC15_1 = BatchNormalization()(TC15_1)\n",
    "    TC15_2 = Conv2D(512, (11,1), padding='same')(P7)\n",
    "    TC15_2 = PReLU()(TC15_2)\n",
    "    TC15_2 = BatchNormalization()(TC15_2)\n",
    "#BloqueD\n",
    "    P8 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_1)\n",
    "    P9 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_2)\n",
    "    P10 = AveragePooling2D((41,1), strides=16, padding='same')(LCI)\n",
    "    TC16 = Conv2D(32, (5,1), padding='same', activation='tanh')(P10)\n",
    "    TC16 = BatchNormalization()(TC16)\n",
    "    TC17 = Conv2D(32, (11,1), padding='same', activation='tanh')(P10)\n",
    "    TC17 = BatchNormalization()(TC17)\n",
    "    TC18 = Conv2D(32, (21,1), padding='same', activation='tanh')(P10)\n",
    "    TC18 = BatchNormalization()(TC18)\n",
    "    C8  = concatenate([TC16,TC17,TC18])\n",
    "    TC19 = Conv2D(64, (11,1), padding='same')(C8)\n",
    "    TC19 = PReLU()(TC19)\n",
    "    TC19 = BatchNormalization()(TC19)\n",
    "    MC3 = Conv2D(64, (1, 5), padding='same')(TC19)\n",
    "    MC3 = PReLU()(MC3)\n",
    "    MC3 = BatchNormalization()(MC3)\n",
    "    C9  = concatenate([P8,MC3])\n",
    "    C10  = concatenate([P9,TC19])\n",
    "    TC20_1 = Conv2D(128, (11,1), padding='same')(C9)\n",
    "    TC20_1 = PReLU()(TC20_1)\n",
    "    TC20_1 = BatchNormalization()(TC20_1)\n",
    "    TC20_2 = Conv2D(128, (11,1), padding='same')(C10)\n",
    "    TC20_2 = PReLU()(TC20_2)\n",
    "    TC20_2 = BatchNormalization()(TC20_2)\n",
    "#BloqueE\n",
    "    P11 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_1)\n",
    "    P12 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_2)\n",
    "    P13 = AveragePooling2D((61,1), strides=32, padding='same')(LCI)\n",
    "    TC21 = Conv2D(256, (5,1), padding='same', activation='tanh')(P13)\n",
    "    TC21 = BatchNormalization()(TC21)\n",
    "    TC22 = Conv2D(256, (11,1), padding='same', activation='tanh')(P13)\n",
    "    TC22 = BatchNormalization()(TC22)\n",
    "    TC23 = Conv2D(256, (21,1), padding='same', activation='tanh')(P13)\n",
    "    TC23 = BatchNormalization()(TC23)\n",
    "    C11  = concatenate([TC21,TC22,TC23])   \n",
    "    TC24 = Conv2D(512, (11,1), padding='same')(C11)  \n",
    "    TC24 = PReLU()(TC24)\n",
    "    TC24 = BatchNormalization()(TC24)\n",
    "    C12  = concatenate([P12,TC24])       \n",
    "    TC25 = Conv2D(512, (11,1), padding='same')(C12)\n",
    "    TC25 = PReLU()(TC25)\n",
    "    TC25 = BatchNormalization()(TC25)\n",
    "    TC26 = Conv2D(512, (21,1), padding='same')(C12)\n",
    "    TC26 = PReLU()(TC26)\n",
    "    TC26 = BatchNormalization()(TC26)\n",
    "    C13  = concatenate([TC25,TC26])  \n",
    "    MC4 = Conv2D(1024, (1, 5), padding='same')(C13)\n",
    "    MC4 = PReLU()(MC4)\n",
    "    MC4 = BatchNormalization()(MC4)\n",
    "    C14  = concatenate([P11,MC4]) \n",
    "    TC27 = Conv2D(2048, (5,1), padding='same')(C14)\n",
    "    TC27 = PReLU()(TC27)\n",
    "    TC27 = BatchNormalization()(TC27)\n",
    "    TC28 = Conv2D(2048, (11,1), padding='same')(C14)\n",
    "    TC28 = PReLU()(TC28)\n",
    "    TC28 = BatchNormalization()(TC28)\n",
    "    TC29 = Conv2D(2048, (21,1), padding='same')(C14)\n",
    "    TC29 = PReLU()(TC29)\n",
    "    TC29 = BatchNormalization()(TC29)\n",
    "    TC30 = Conv2D(2048, (41,1), padding='same')(C14) \n",
    "    TC30 = PReLU()(TC30)\n",
    "    TC30 = BatchNormalization()(TC30)\n",
    "    C15  = concatenate([TC27,TC28,TC29,TC30]) \n",
    "#FullyConectedLayers\n",
    "    FC1 = Flatten()(C15)\n",
    "    merged_1 = Dense(8192, activation='relu')(FC1)\n",
    "    merged_2 = Dropout(0.5)(merged_1)\n",
    "    SM = Dense(2, activation='softmax')(merged_2)\n",
    "#Entrenamiento\n",
    "    model = Model(LCI, SM)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit_generator(entrenamiento_generador, \n",
    "                             steps_per_epoch = 10,\n",
    "                             epochs = 2,\n",
    "                             validation_data = validacion_generador,\n",
    "                             validation_steps = 10)\n",
    "    return model\n",
    "print(cnn(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-analysis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-neighbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-louisville",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-estate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-netherlands",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-player",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-prague",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-begin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-bouquet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 100, 15\n",
    "def create_convnet(img_path='network_image.png'):\n",
    "    input_shape = Input(shape=(rows, cols, 1))\n",
    "\n",
    "    tower_1 = Conv2D(20, (100, 5), padding='same', activation='relu')(input_shape)\n",
    "    tower_1 = MaxPooling2D((1, 11), strides=(1, 1), padding='same')(tower_1)\n",
    "\n",
    "    tower_2 = Conv2D(20, (100, 7), padding='same', activation='relu')(input_shape)\n",
    "    tower_2 = MaxPooling2D((1, 9), strides=(1, 1), padding='same')(tower_2)\n",
    "\n",
    "    tower_3 = Conv2D(20, (100, 10), padding='same', activation='relu')(input_shape)\n",
    "    tower_3 = MaxPooling2D((1, 6), strides=(1, 1), padding='same')(tower_3)\n",
    "\n",
    "    merged_1 = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "    merged_1 = Flatten()(merged_1)\n",
    "\n",
    "    out = Dense(200, activation='relu')(merged)\n",
    "    out = Dense(num_classes, activation='softmax')(out)\n",
    "\n",
    "    model = Model(input_shape, out)\n",
    "    plot_model(model, to_file=img_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = Input(shape=(longitud,altura, 3))\n",
    "\n",
    "tower_1 = Conv2D(20, (100, 5), padding='same', activation='relu')(input_shape)\n",
    "tower_1 = MaxPooling2D((1, 11), strides=(1, 1), padding='same')(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(20, (100, 7), padding='same', activation='relu')(input_shape)\n",
    "tower_2 = MaxPooling2D((1, 9), strides=(1, 1), padding='same')(tower_2)\n",
    "\n",
    "tower_3 = Conv2D(20, (100, 10), padding='same', activation='relu')(input_shape)\n",
    "tower_3 = MaxPooling2D((1, 6), strides=(1, 1), padding='same')(tower_3)\n",
    "\n",
    "merged_1 = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "merged_1 = Flatten()(merged_1)\n",
    "\n",
    "out = Dense(200, activation='relu')(merged)\n",
    "out = Dense(num_classes, activation='softmax')(out)\n",
    "\n",
    "model = Model(input_shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "integral-professional",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8ccd2f06fe78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0min1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongitud\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maltura\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltrosConv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamano_filtro1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtamano_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m               \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             inputs, outputs = self._set_connectivity_metadata_(\n\u001b[0;32m--> 874\u001b[0;31m                 inputs, outputs, args, kwargs)\n\u001b[0m\u001b[1;32m    875\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_set_connectivity_metadata_\u001b[0;34m(self, inputs, outputs, args, kwargs)\u001b[0m\n\u001b[1;32m   2036\u001b[0m     \u001b[0;31m# This updates the layer history of the output tensor(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m     self._add_inbound_node(\n\u001b[0;32m-> 2038\u001b[0;31m         input_tensors=inputs, output_tensors=outputs, arguments=arguments)\n\u001b[0m\u001b[1;32m   2039\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_add_inbound_node\u001b[0;34m(self, input_tensors, output_tensors, arguments)\u001b[0m\n\u001b[1;32m   2052\u001b[0m     \"\"\"\n\u001b[1;32m   2053\u001b[0m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0;32m-> 2054\u001b[0;31m                                         input_tensors)\n\u001b[0m\u001b[1;32m   2055\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n\u001b[1;32m   2056\u001b[0m                                       input_tensors)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   2051\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m     \"\"\"\n\u001b[0;32m-> 2053\u001b[0;31m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0m\u001b[1;32m   2054\u001b[0m                                         input_tensors)\n\u001b[1;32m   2055\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "in1 = Input(shape=(longitud,altura, 3))\n",
    "x=(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(5, 1700, 3), activation='relu'))(in1)\n",
    "x=(MaxPooling2D(pool_size=tamano_pool))(x)\n",
    "x=(Dropout(0.5))(x)\n",
    "\n",
    "x=(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))(x)\n",
    "x=(MaxPooling2D(pool_size=tamano_pool))(x)\n",
    "x=(Dropout(0.25))(x)\n",
    "\n",
    "x=(Flatten())(x)\n",
    "x=(Dense(256, activation='relu'))(x)\n",
    "x=(Dropout(0.5))(x)\n",
    "out1=(Dense(clases, activation='softmax'))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = Input(shape=(longitud,altura, 3))\n",
    "x=(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(5, 1700, 3), activation='relu'))(in1)\n",
    "x=(MaxPooling2D(pool_size=tamano_pool))(x)\n",
    "x=(Dropout(0.5))(x)\n",
    "\n",
    "x=(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))(x)\n",
    "x=(MaxPooling2D(pool_size=tamano_pool))(x)\n",
    "x=(Dropout(0.25))(x)\n",
    "\n",
    "x=(Flatten())(x)\n",
    "x=(Dense(256, activation='relu'))(x)\n",
    "x=(Dropout(0.5))(x)\n",
    "out1=(Dense(clases, activation='softmax'))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "color-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = Sequential()\n",
    "cnn1.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(5, 1700, 3), activation='relu'))\n",
    "cnn1.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn1.add(Dropout(0.5))\n",
    "\n",
    "cnn1.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))\n",
    "cnn1.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn1.add(Dropout(0.25))\n",
    "\n",
    "cnn1.add(Flatten())\n",
    "cnn1.add(Dense(256, activation='relu'))\n",
    "cnn1.add(Dropout(0.5))\n",
    "cnn1.add(Dense(clases, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "horizontal-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = Sequential()\n",
    "cnn2.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(5, 1700, 3), activation='relu'))\n",
    "cnn2.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn2.add(Dropout(0.5))\n",
    "\n",
    "cnn2.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))\n",
    "cnn2.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn2.add(Dropout(0.25))\n",
    "\n",
    "cnn2.add(Flatten())\n",
    "cnn2.add(Dense(256, activation='relu'))\n",
    "cnn2.add(Dropout(0.5))\n",
    "cnn2.add(Dense(clases, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "assigned-favor",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Concatenate' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-66692fdc3000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2122\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2123\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         spec.max_ndim is not None):\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m    165\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Concatenate' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "input_shape = Input(shape=(5, 1700, 3))\n",
    "merged = Concatenate((cnn1, cnn2))\n",
    "merged = Flatten()(merged)\n",
    "\n",
    "out = Dense(200, activation='relu')(merged)\n",
    "out = Dense(num_classes, activation='softmax')(out)\n",
    "\n",
    "model = Model(input_shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "respective-administrator",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Concatenate' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-700e5a7707aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2122\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2123\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         spec.max_ndim is not None):\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m    165\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Concatenate' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "combined = Concatenate([cnn1.output, cnn2.output])\n",
    "\n",
    "out = Dense(128, activation='relu')(combined)\n",
    "model = Model(inputs = [cnn1.input, cnn2.input], outputs = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ready-witch",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Concatenate' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-eecdb4b9b458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcnn3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcnn3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m cnn3.fit_generator(entrenamiento_generador,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2122\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2123\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         spec.max_ndim is not None):\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m    165\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Concatenate' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x=Concatenate([out1, out2])\n",
    "Dense(clases, activation='softmax')(x)\n",
    "cnn3 = Model(inputs=[in1, in2], outputs=[out])\n",
    "cnn3.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "cnn3.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Concatenate([out1, out2])\n",
    "out = Dense(clases, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "developmental-antenna",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prescribed-failing",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d1165193d25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "new_model = keras.Model(inputs=[cnn1, cnn2], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "egyptian-point",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input tensors to a Model must come from `keras.layers.Input`. Received: <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fe93f657210> (missing previous layer metadata).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-06eadc06c539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                                  \u001b[0;34m'must come from `keras.layers.Input`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                                  \u001b[0;34m'Received: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                                  ' (missing previous layer metadata).')\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;31m# Check that x is an input tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input tensors to a Model must come from `keras.layers.Input`. Received: <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fe93f657210> (missing previous layer metadata)."
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[cnn1, cnn2], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-wings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-birth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-moses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-brisbane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-sheep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-brazilian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-replication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-cathedral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-saturn",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "french-chamber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 71s 7s/step - loss: 14.1716 - accuracy: 0.0758 - val_loss: 14.4969 - val_accuracy: 0.0545\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f90aeb4f250>\n"
     ]
    }
   ],
   "source": [
    "#prueba\n",
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    batch_size = 33\n",
    "    pasos = 1000\n",
    "    validation_steps = 300\n",
    "    filtrosConv1 = 31\n",
    "    filtrosConv2 = 64\n",
    "    tamano_filtro1 = (32, 3)\n",
    "    tamano_filtro2 = (2, 2)\n",
    "    tamano_pool = (2, 2)\n",
    "\n",
    "    input_shape = Input(shape=(5,1700,3))\n",
    "\n",
    "    tower_1 = Conv2D(20, (100, 5), padding='same', activation='tanh')(input_shape)\n",
    "    tower_1 = MaxPooling2D((1, 11), strides=2, padding='same')(tower_1)\n",
    "\n",
    "    tower_2 = Conv2D(20, (100, 7), padding='same', activation='tanh')(input_shape)\n",
    "    tower_2 = MaxPooling2D((1, 9), strides=2, padding='same')(tower_2)\n",
    "    \n",
    "    merged = concatenate([tower_1, tower_2])\n",
    "    merged1 = Flatten()(merged)\n",
    "    \n",
    "    out1 = Dense(1)(merged1) \n",
    "    out2 = PReLU()(out1)\n",
    "    out3 = Dense(1, activation='softmax')(out2)\n",
    "\n",
    "    model = Model(input_shape, out3)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit_generator(entrenamiento_generador, \n",
    "                             steps_per_epoch = 10,\n",
    "                             epochs = 1,\n",
    "                             validation_data = validacion_generador,\n",
    "                             validation_steps = 10)\n",
    "    return model\n",
    "print(cnn(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "introductory-welcome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 54s 5s/step - loss: 14.5433 - accuracy: 0.0515 - val_loss: 14.4969 - val_accuracy: 0.0545\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f90593fc3d0>\n"
     ]
    }
   ],
   "source": [
    "#prueba\n",
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    batch_size = 33\n",
    "    pasos = 1000\n",
    "    validation_steps = 300\n",
    "    filtrosConv1 = 31\n",
    "    filtrosConv2 = 64\n",
    "    tamano_filtro1 = (32, 3)\n",
    "    tamano_filtro2 = (2, 2)\n",
    "    tamano_pool = (2, 2)\n",
    "\n",
    "    input_shape = Input(shape=(5,1700,3))\n",
    "\n",
    "    tower_1 = Conv2D(20, (100, 5), padding='same', activation='tanh')(input_shape)\n",
    "    tower_1 = MaxPooling2D((1, 11), strides=2, padding='same')(tower_1)\n",
    "    tower_1 = BatchNormalization()(tower_1)\n",
    "    \n",
    "    tower_2 = Conv2D(20, (100, 7), padding='same', activation='tanh')(input_shape)\n",
    "    tower_2 = MaxPooling2D((1, 9), strides=2, padding='same')(tower_2)\n",
    "    \n",
    "    merged = concatenate([tower_1, tower_2])\n",
    "    merged = Flatten()(merged)\n",
    "    \n",
    "    out1 = Dense(1, activation='relu')(merged)    \n",
    "    out3 = Dense(1, activation='softmax')(out1)\n",
    "\n",
    "    model = Model(input_shape, out3)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit_generator(entrenamiento_generador, \n",
    "                             steps_per_epoch = 10,\n",
    "                             epochs = 1,\n",
    "                             validation_data = validacion_generador,\n",
    "                             validation_steps = 10)\n",
    "    return model\n",
    "print(cnn(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "competent-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 77s 8s/step - loss: 14.3110 - accuracy: 0.0667 - val_loss: 14.4969 - val_accuracy: 0.0545\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f9065b35550>\n"
     ]
    }
   ],
   "source": [
    "#prueba\n",
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    batch_size = 33\n",
    "    pasos = 1000\n",
    "    validation_steps = 300\n",
    "    filtrosConv1 = 31\n",
    "    filtrosConv2 = 64\n",
    "    tamano_filtro1 = (32, 3)\n",
    "    tamano_filtro2 = (2, 2)\n",
    "    tamano_pool = (2, 2)\n",
    "\n",
    "    input_shape = Input(shape=(5,1700,3))\n",
    "\n",
    "    tower_1 = Conv2D(20, (100, 5), padding='same', activation='tanh')(input_shape)\n",
    "    tower_1 = MaxPooling2D((1, 11), strides=2, padding='same')(tower_1)\n",
    "\n",
    "    tower_2 = Conv2D(20, (100, 7), padding='same', activation='tanh')(input_shape)\n",
    "    tower_2 = MaxPooling2D((1, 9), strides=2, padding='same')(tower_2)\n",
    "    \n",
    "    merged = concatenate([tower_1, tower_2])\n",
    "    merged1 = Flatten()(merged)\n",
    "    \n",
    "    out1 = BatchNormalization(axis=1)(merged1)\n",
    "    out2 = Dense(1, activation='relu')(out1)\n",
    "    out4 = Dense(1, activation='softmax')(out2)  \n",
    "\n",
    "    model = Model(input_shape, out4)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit_generator(entrenamiento_generador, \n",
    "                             steps_per_epoch = 10,\n",
    "                             epochs = 1,\n",
    "                             validation_data = validacion_generador,\n",
    "                             validation_steps = 10)\n",
    "    return model\n",
    "print(cnn(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    LCI = Input(shape=(5,1700,3))\n",
    "#BloqueA  \n",
    "    P1  = AveragePooling2D((5,1), strides=2, padding='same')(LCI)\n",
    "    TC1 = Conv2D(32, (11,1), padding='same', activation='tanh')(P1)\n",
    "    TC1 = BatchNormalization()(TC1)\n",
    "    TC2 = Conv2D(32, (21,1), padding='same', activation='tanh')(P1)\n",
    "    TC2 = BatchNormalization()(TC2)\n",
    "    TC3 = Conv2D(32, (41,1), padding='same', activation='tanh')(P1)\n",
    "    TC3 = BatchNormalization()(TC3)\n",
    "    P2  = AveragePooling2D((11,1), strides=2, padding='same')(LCI)\n",
    "    TC4 = Conv2D(32, (11,1), padding='same', activation='tanh')(P2)\n",
    "    TC4 = BatchNormalization()(TC4)\n",
    "    TC5 = Conv2D(32, (21,1), padding='same', activation='tanh')(P2)\n",
    "    TC5 = BatchNormalization()(TC5)\n",
    "    TC6 = Conv2D(32, (41,1), padding='same', activation='tanh')(P2)\n",
    "    TC6 = BatchNormalization()(TC6)\n",
    "    C1  = concatenate([TC1,TC2,TC3,TC4,TC5,TC6])\n",
    "    MC1 = Conv2D(64, (1, 5), padding='same')(C1)\n",
    "    MC1 = PReLU()(MC1)\n",
    "    MC1 = BatchNormalization()(MC1) \n",
    "    TC7_1 = Conv2D(64, (11,1), padding='same')(C1)\n",
    "    TC7_1 = PReLU()(TC7_1)\n",
    "    TC7_1 = BatchNormalization()(TC7_1)\n",
    "    TC8_1 = Conv2D(64, (21,1), padding='same')(C1)\n",
    "    TC8_1 = PReLU()(TC8_1)\n",
    "    TC8_1 = BatchNormalization()(TC8_1)\n",
    "    TC7_2 = Conv2D(64, (11,1), padding='same')(MC1)\n",
    "    TC7_2 = PReLU()(TC7_2)\n",
    "    TC7_2 = BatchNormalization()(TC7_2)\n",
    "    TC8_2 = Conv2D(64, (21,1), padding='same')(MC1)\n",
    "    TC8_2 = PReLU()(TC8_2)\n",
    "    TC8_2 = BatchNormalization()(TC8_2)\n",
    "    C2  = concatenate([TC7_1,TC8_1])\n",
    "    C3  = concatenate([TC7_2,TC8_2])\n",
    "#BloqueB\n",
    "    P3  = MaxPooling2D((3,1), strides=2, padding='same')(C2)\n",
    "    P4  = MaxPooling2D((3,1), strides=2, padding='same')(C3)\n",
    "    P5  = AveragePooling2D((21,1), strides=4, padding='same')(LCI)\n",
    "    TC9 = Conv2D(32, (5,1), padding='same', activation='tanh')(P5)\n",
    "    TC9 = BatchNormalization()(TC9)\n",
    "    TC10 = Conv2D(32, (11,1), padding='same', activation='tanh')(P5)\n",
    "    TC10 = BatchNormalization()(TC10)\n",
    "    TC11 = Conv2D(32, (21,1), padding='same', activation='tanh')(P5) \n",
    "    TC11 = BatchNormalization()(TC11)\n",
    "    C4  = concatenate([TC9,TC10,TC11])\n",
    "    TC12 = Conv2D(64, (11,1), padding='same')(C4)\n",
    "    TC12 = PReLU()(TC12)\n",
    "    TC12 = BatchNormalization()(TC12)\n",
    "    TC13 = Conv2D(64, (21,1), padding='same')(C4)\n",
    "    TC13 = PReLU()(TC13)\n",
    "    TC13 = BatchNormalization()(TC13)\n",
    "    C5  = concatenate([TC12,TC13])\n",
    "    MC2 = Conv2D(128, (1, 5), padding='same')(C5)\n",
    "    MC2 = PReLU()(MC2)\n",
    "    MC2 = BatchNormalization()(MC2)\n",
    "    C6  = concatenate([P3,MC2])\n",
    "    C7  = concatenate([P4,C5])\n",
    "    TC14_1 = Conv2D(256, (11,1), padding='same')(C6)\n",
    "    TC14_1 = PReLU()(TC14_1)\n",
    "    TC14_1 = BatchNormalization()(TC14_1)\n",
    "    TC14_2 = Conv2D(256, (11,1), padding='same')(C7)\n",
    "    TC14_2 = PReLU()(TC14_2)\n",
    "    TC14_2 = BatchNormalization()(TC14_2)\n",
    "#BloqueC\n",
    "    P6 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_1)\n",
    "    P7 = MaxPooling2D((3,1), strides=2, padding='same')(TC14_2)\n",
    "    TC15_1 = Conv2D(512, (11,1), padding='same')(P6)\n",
    "    TC15_1 = PReLU()(TC15_1)\n",
    "    TC15_1 = BatchNormalization()(TC15_1)\n",
    "    TC15_2 = Conv2D(512, (11,1), padding='same')(P7)\n",
    "    TC15_2 = PReLU()(TC15_2)\n",
    "    TC15_2 = BatchNormalization()(TC15_2)\n",
    "#BloqueD\n",
    "    P8 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_1)\n",
    "    P9 = MaxPooling2D((3,1), strides=2, padding='same')(TC15_2)\n",
    "    P10 = AveragePooling2D((41,1), strides=16, padding='same')(LCI)\n",
    "    TC16 = Conv2D(32, (5,1), padding='same', activation='tanh')(P10)\n",
    "    TC16 = BatchNormalization()(TC16)\n",
    "    TC17 = Conv2D(32, (11,1), padding='same', activation='tanh')(P10)\n",
    "    TC17 = BatchNormalization()(TC17)\n",
    "    TC18 = Conv2D(32, (21,1), padding='same', activation='tanh')(P10)\n",
    "    TC18 = BatchNormalization()(TC18)\n",
    "    C8  = concatenate([TC16,TC17,TC18])\n",
    "    TC19 = Conv2D(64, (11,1), padding='same')(C8)\n",
    "    TC19 = PReLU()(TC19)\n",
    "    TC19 = BatchNormalization()(TC19)\n",
    "    MC3 = Conv2D(64, (1, 5), padding='same')(TC19)\n",
    "    MC3 = PReLU()(MC3)\n",
    "    MC3 = BatchNormalization()(MC3)\n",
    "    C9  = concatenate([P8,MC3])\n",
    "    C10  = concatenate([P9,TC19])\n",
    "    TC20_1 = Conv2D(128, (11,1), padding='same')(C9)\n",
    "    TC20_1 = PReLU()(TC20_1)\n",
    "    TC20_1 = BatchNormalization()(TC20_1)\n",
    "    TC20_2 = Conv2D(128, (11,1), padding='same')(C10)\n",
    "    TC20_2 = PReLU()(TC20_2)\n",
    "    TC20_2 = BatchNormalization()(TC20_2)\n",
    "#BloqueE\n",
    "    P11 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_1)\n",
    "    P12 = MaxPooling2D((3,1), strides=2, padding='same')(TC20_2)\n",
    "    P13 = AveragePooling2D((61,1), strides=32, padding='same')(LCI)\n",
    "    TC21 = Conv2D(256, (5,1), padding='same', activation='tanh')(P13)\n",
    "    TC21 = BatchNormalization()(TC21)\n",
    "    TC22 = Conv2D(256, (11,1), padding='same', activation='tanh')(P13)\n",
    "    TC22 = BatchNormalization()(TC22)\n",
    "    TC23 = Conv2D(256, (21,1), padding='same', activation='tanh')(P13)\n",
    "    TC23 = BatchNormalization()(TC23)\n",
    "    C11  = concatenate([TC21,TC22,TC23])   \n",
    "    TC24 = Conv2D(512, (11,1), padding='same')(C11)  \n",
    "    TC24 = PReLU()(TC24)\n",
    "    TC24 = BatchNormalization()(TC24)\n",
    "    C12  = concatenate([P12,TC24])       \n",
    "    TC25 = Conv2D(512, (11,1), padding='same')(C12)\n",
    "    TC25 = PReLU()(TC25)\n",
    "    TC25 = BatchNormalization()(TC25)\n",
    "    TC26 = Conv2D(512, (21,1), padding='same')(C12)\n",
    "    TC26 = PReLU()(TC26)\n",
    "    TC26 = BatchNormalization()(TC26)\n",
    "    C13  = concatenate([TC25,TC26])  \n",
    "    MC4 = Conv2D(1024, (1, 5), padding='same')(C13)\n",
    "    MC4 = PReLU()(MC4)\n",
    "    MC4 = BatchNormalization()(MC4)\n",
    "    C14  = concatenate([P11,MC4]) \n",
    "    TC27 = Conv2D(2048, (5,1), padding='same')(C14)\n",
    "    TC27 = PReLU()(TC27)\n",
    "    TC27 = BatchNormalization()(TC27)\n",
    "    TC28 = Conv2D(2048, (11,1), padding='same')(C14)\n",
    "    TC28 = PReLU()(TC28)\n",
    "    TC28 = BatchNormalization()(TC28)\n",
    "    TC29 = Conv2D(2048, (21,1), padding='same')(C14)\n",
    "    TC29 = PReLU()(TC29)\n",
    "    TC29 = BatchNormalization()(TC29)\n",
    "    TC30 = Conv2D(2048, (41,1), padding='same')(C14) \n",
    "    TC30 = PReLU()(TC30)\n",
    "    TC30 = BatchNormalization()(TC30)\n",
    "    C15  = concatenate([TC27,TC28,TC29,TC30]) \n",
    "#FullyConectedLayers\n",
    "    FC1 = Flatten()(C15)\n",
    "    merged_1 = Dense(8192, activation='relu')(FC1)\n",
    "    merged_2 = Dropout(0.5)(merged_1)\n",
    "    FC2 = Flatten()(merged_2)\n",
    "    merged_3 = Dense(8192, activation='relu')(FC2) \n",
    "    merged_4 = Dropout(0.5)(merged3)\n",
    "    SM = Dense(2, activation='softmax')(merged_4)\n",
    "#Entrenamiento\n",
    "    model = Model(LCI, SM)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit_generator(entrenamiento_generador, \n",
    "                             steps_per_epoch = int(7335/float(batch_size)),\n",
    "                             epochs = 2,\n",
    "                             validation_data = validacion_generador,\n",
    "                             validation_steps = int(2443/float(batch_size)))\n",
    "    return model\n",
    "print(cnn(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "subsequent-phoenix",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-60a4b49facd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model.fit_generator(entrenamiento_generador,\n\u001b[1;32m      3\u001b[0m                          \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidacion_generador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "synthetic-villa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2b3e633be623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-2b3e633be623>\u001b[0m in \u001b[0;36mcnn\u001b[0;34m(rows, cols)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtower_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltrosConv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamano_filtro1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtower_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtamano_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtower_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtower_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtower_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    batch_size = 33\n",
    "    pasos = 1000\n",
    "    validation_steps = 300\n",
    "    filtrosConv1 = 31\n",
    "    filtrosConv2 = 64\n",
    "    tamano_filtro1 = (32, 3)\n",
    "    tamano_filtro2 = (2, 2)\n",
    "    tamano_pool = (2, 2)\n",
    "\n",
    "    input_shape = Input(shape=(rows, cols, 3))\n",
    "\n",
    "    tower_1 = (Conv2D(filtrosConv1, tamano_filtro1, padding =\"same\", activation='relu'))(input_shape)\n",
    "    tower_1 = (MaxPooling2D(pool_size=tamano_pool))(tower_1)\n",
    "    tower_1 = (Dropout(0.5))(tower_1)\n",
    "    \n",
    "    tower_2 = (keras.layers.Conv2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))(input_shape)\n",
    "    tower_2 = (MaxPooling2D(pool_size=tamano_pool))(tower_2)\n",
    "    tower_2 = (Dropout(0.25))(tower_2)\n",
    "    \n",
    "    merged_1 = keras.layers.concatenate([tower_1, tower_2], axis=1)\n",
    "    merged_1 = Flatten()(merged_1)\n",
    "    merged_1 = Dense(256, activation='relu')(merged_1)\n",
    "    merged_1 = Dropout(0.5)(merged_1)\n",
    "    \n",
    "    out = Dense(200, activation='relu')(merged_1)\n",
    "    out = Dense(num_classes, activation='softmax')(out)   \n",
    "    return model\n",
    "print(cnn(rows, cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mechanical-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 1700\n",
    "def cnn(rows, cols):\n",
    "    input_shape = Input(shape=(rows, cols, 3))\n",
    "\n",
    "    tower_1 = (Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(5, 1700, 3), activation='relu'))\n",
    "    tower_1 = (MaxPooling2D(pool_size=tamano_pool))(tower_1)\n",
    "    tower_1 = (Dropout(0.5))(tower_1)\n",
    "    \n",
    "    tower_2 = (Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))\n",
    "    tower_2 = (MaxPooling2D(pool_size=tamano_pool))(tower_2)\n",
    "    tower_2 = (Dropout(0.25))(tower_2)\n",
    "    \n",
    "    merged_1 = keras.layers.concatenate([tower_1, tower_2], axis=1)\n",
    "    merged_1 = Flatten()(merged_1)\n",
    "    merged_1 = Dense(256, activation='relu')(merged_1)\n",
    "    merged_1 = Dropout(0.5)(merged_1)\n",
    "    \n",
    "    out = Dense(200, activation='relu')(merged_1)\n",
    "    out = Dense(num_classes, activation='softmax')(out)   \n",
    "    cnn = compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])(out)\n",
    "    fit = fit_generator(entrenamiento_generador,steps_per_epoch = 5,epochs = 2,validation_data = validacion_generador,validation_steps = 5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bearing-location",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-60a4b49facd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model.fit_generator(entrenamiento_generador,\n\u001b[1;32m      3\u001b[0m                          \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidacion_generador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "handed-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(5, 1700, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn.add(Dropout(0.5))\n",
    "\n",
    "cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(clases, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "separate-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5/5 [==============================] - 12s 2s/step - loss: 14.3110 - accuracy: 0.0667 - val_loss: 14.9615 - val_accuracy: 0.0242\n",
      "Epoch 2/3\n",
      "5/5 [==============================] - 10s 2s/step - loss: 14.5898 - accuracy: 0.0485 - val_loss: 14.9615 - val_accuracy: 0.0242\n",
      "Epoch 3/3\n",
      "5/5 [==============================] - 10s 2s/step - loss: 14.9615 - accuracy: 0.0242 - val_loss: 14.9615 - val_accuracy: 0.0242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb038c337d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 3,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "single-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a06a0b2d0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "italic-fancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 12s 2s/step - loss: 1.0000 - accuracy: 0.0000e+00 - val_loss: 1.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 14s 3s/step - loss: 1.0000 - accuracy: 0.0000e+00 - val_loss: 1.0000 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89d6364310>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "median-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 13s 3s/step - loss: 1.0000 - accuracy: 0.0000e+00 - val_loss: 1.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0000 - accuracy: 0.0000e+00 - val_loss: 1.0000 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89d6b7d2d0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(optimizer=keras.optimizers.Adam(),loss=tf.keras.losses.Poisson(reduction=\"auto\", name=\"poisson\"),\n",
    "            metrics=['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "direct-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 13s 3s/step - loss: 1.3133 - accuracy: 0.0000e+00 - val_loss: 1.3133 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.3133 - accuracy: 0.0000e+00 - val_loss: 1.3133 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89d7533f90>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(optimizer=keras.optimizers.Adam(),loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "handed-oliver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 12s 2s/step - loss: 15.3332 - accuracy: 0.0000e+00 - val_loss: 15.3332 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 11s 2s/step - loss: 15.3332 - accuracy: 0.0000e+00 - val_loss: 15.3332 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89d5c27350>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(loss = keras.losses.binary_crossentropy, optimizer = 'sgd', metrics = ['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-diving",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "laden-deployment",
   "metadata": {},
   "source": [
    "# Prueba simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ahead-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas=20\n",
    "longitud, altura = 5, 1700\n",
    "batch_size = 32\n",
    "pasos = 1000\n",
    "validation_steps = 300\n",
    "filtrosConv1 = 32\n",
    "filtrosConv2 = 64\n",
    "tamano_filtro1 = (3, 3)\n",
    "tamano_filtro2 = (2, 2)\n",
    "tamano_pool = (2, 2)\n",
    "clases = 3\n",
    "lr = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "joined-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(5, 1700, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn.add(Dropout(0.5))\n",
    "\n",
    "cnn.add(Conv2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "alternate-narrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 124s 12s/step - loss: 14.2181 - accuracy: 0.0727 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 119s 12s/step - loss: 14.4969 - accuracy: 0.0545 - val_loss: 14.4907 - val_accuracy: 0.0491\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 108s 11s/step - loss: 14.3575 - accuracy: 0.0636 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 107s 11s/step - loss: 14.4969 - accuracy: 0.0545 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 104s 10s/step - loss: 14.3783 - accuracy: 0.0588 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 119s 12s/step - loss: 14.5898 - accuracy: 0.0485 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 127s 13s/step - loss: 14.7292 - accuracy: 0.0394 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 119s 12s/step - loss: 14.4040 - accuracy: 0.0606 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 112s 11s/step - loss: 14.5898 - accuracy: 0.0485 - val_loss: 14.5898 - val_accuracy: 0.0491\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 104s 10s/step - loss: 14.3110 - accuracy: 0.0667 - val_loss: 14.5898 - val_accuracy: 0.0491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd10f917110>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 10,\n",
    "                         epochs = 10,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "tired-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/richo/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /Volumes/ADATA/model.h5py/assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save('/Volumes/ADATA/model.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "multiple-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "satisfied-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = load_model('/Volumes/ADATA/model.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "consistent-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file):\n",
    "  x = load_img(file, target_size=(5,1700))\n",
    "  x = img_to_array(x)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  array = pred.predict(x)\n",
    "  result = array[0]\n",
    "  answer = np.argmax(result)\n",
    "  if answer == 0:\n",
    "    print(\"pred: Quasar\")\n",
    "  elif anser==1:\n",
    "    print(\"pred: RRLyrae\")\n",
    "  else:\n",
    "    print(\"pred: inconcluso\")\n",
    "\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "overhead-disney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Quasar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rrlyrae\n",
    "predict('/Volumes/ADATA/Validaci√≥nIm/RRLyrae/LC_13350_completo_promedio.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "pharmaceutical-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Quasar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quasar\n",
    "predict('/Volumes/ADATA/Validaci√≥nIm/Quasares/LC_1540_completo_promedio.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "above-wiring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Quasar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/r.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "million-rhythm",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_input to have shape (5, 1700, 3) but got array with shape (150, 150, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-abf9bbee7e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/x.tiff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-84785b2e86f8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have shape (5, 1700, 3) but got array with shape (150, 150, 3)"
     ]
    }
   ],
   "source": [
    "predict('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/x.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "racial-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Quasar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/y.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "negative-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Quasar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/z.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "hispanic-minnesota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Quasar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Volumes/ADATA/random/1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "following-operation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Quasar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Volumes/ADATA/random/2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-protocol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-positive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-comfort",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "enabling-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "metric-czech",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 5 from 1 for 'conv2d_16/Conv2D' (op: 'Conv2D') with input shapes: [?,1,849,1700], [5,3,1700,3400].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 5 from 1 for 'conv2d_16/Conv2D' (op: 'Conv2D') with input shapes: [?,1,849,1700], [5,3,1700,3400].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-79ebd800c9fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m   1132\u001b[0m           call_from_convolution=False)\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m     \u001b[0;31m# copybara:strip_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;31m# copybara:insert return self.conv_op(inp, filter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[1;32m   2008\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m                            name=name)\n\u001b[0m\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m   1072\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1773\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1774\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 5 from 1 for 'conv2d_16/Conv2D' (op: 'Conv2D') with input shapes: [?,1,849,1700], [5,3,1700,3400]."
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(1700, (3, 3), activation='relu', input_shape=(5,1700, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(3400, (5, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "swedish-monaco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_2_input to have shape (32, 32, 3) but got array with shape (5, 1700, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6e3ff2730a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidacion_generador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                          validation_steps = 5)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_2_input to have shape (32, 32, 3) but got array with shape (5, 1700, 3)"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 5,\n",
    "                         epochs = 2,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-filename",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-plastic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-hearts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-stage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-toner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-lender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-viewer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-phoenix",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-delhi",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-improvement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-invalid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-convertible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-blues",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "regulation-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entrenamiento = '/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/hola/'\n",
    "data_validacion = '/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/hola/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "crucial-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas=20\n",
    "longitud, altura = 500, 500\n",
    "batch_size = 32\n",
    "pasos = 1000\n",
    "validation_steps = 300\n",
    "filtrosConv1 = 32\n",
    "filtrosConv2 = 64\n",
    "tamano_filtro1 = (3, 3)\n",
    "tamano_filtro2 = (2, 2)\n",
    "tamano_pool = (2, 2)\n",
    "clases = 3\n",
    "lr = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "foster-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 1 classes.\n",
      "Found 800 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "entrenamiento_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validacion_generador = test_datagen.flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "distinguished-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(altura, longitud, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(clases, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classified-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='sgd', loss='mse', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-choir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10 steps, validate for 10 steps\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 344s 34s/step - loss: 0.1111 - accuracy: 0.3281 - val_loss: 0.1111 - val_accuracy: 0.3719\n",
      "Epoch 2/2\n"
     ]
    }
   ],
   "source": [
    "cnn.fit(\n",
    "    entrenamiento_generador,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=2,\n",
    "    validation_data=validacion_generador,\n",
    "    validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "colonial-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "scenic-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "            optimizer=optimizers.Adam(lr=lr),\n",
    "            metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "alone-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (32, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-24bd0a1ee4a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidacion_generador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                          validation_steps = 10)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    715\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    716\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (32, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 10,\n",
    "                         epochs = 3,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "thrown-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "balanced-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (32, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-24bd0a1ee4a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidacion_generador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                          validation_steps = 10)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (32, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "cnn.fit_generator(entrenamiento_generador,\n",
    "                         steps_per_epoch = 10,\n",
    "                         epochs = 3,\n",
    "                         validation_data = validacion_generador,\n",
    "                         validation_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "married-strike",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (32, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-4e629f014c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepocas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidacion_generador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     validation_steps=validation_steps)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (32, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "cnn.fit_generator(\n",
    "    entrenamiento_generador,\n",
    "    steps_per_epoch=pasos,\n",
    "    epochs=epocas,\n",
    "    validation_data=validacion_generador,\n",
    "    validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "egyptian-creator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (5, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-208a480cc8d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidacion_generador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     validation_steps=10)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    990\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m    991\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    715\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    716\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (5, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "cnn.fit(\n",
    "    entrenamiento_generador,\n",
    "    steps_per_epoch=10,\n",
    "    epochs=2,\n",
    "    validation_data=validacion_generador,\n",
    "    validation_steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "computational-liabilities",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/richo/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/Prueba/model.h5py/assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/Prueba/model.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eastern-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dependent-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/Prueba/model.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "challenging-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file):\n",
    "  x = load_img(file, target_size=(altura, longitud))\n",
    "  x = img_to_array(x)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  array = model.predict(x)\n",
    "  result = array[0]\n",
    "  answer = np.argmax(result)\n",
    "  if answer == 0:\n",
    "    print(\"pred: avion\")\n",
    "  else:\n",
    "    print(\"pred: No avion\")\n",
    "    \n",
    "  return answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "automated-building",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: avion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Users/richo/Downloads/_105322381_airbus_getty.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afraid-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: No avion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Users/richo/Downloads/gato-marron_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "grateful-somalia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: avion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('/Users/richo/Downloads/seagull-633461_960_720.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-wealth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-bradley",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "disabled-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1700)\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "a = [20,20,20,20,20]\n",
    "for i in range(0,1700):\n",
    "    y.append(a)\n",
    "y = np.array(y,dtype=np.float64,order=\"C\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "img = Image.fromarray(y)\n",
    "print(img.size)\n",
    "img.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/r.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "biological-rouge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1700)\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "a = [0,0,0,0,0]\n",
    "for i in range(0,1700):\n",
    "    y.append(a)\n",
    "y = np.array(y,dtype=np.float64,order=\"C\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "img = Image.fromarray(y)\n",
    "print(img.size)\n",
    "img.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/y.tiff')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "active-spice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1700)\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "a = [255,255,255,255,255]\n",
    "for i in range(0,1700):\n",
    "    x.append(a)\n",
    "x = np.array(x,dtype=np.float64,order=\"C\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "img = Image.fromarray(x)\n",
    "print(img.size)\n",
    "img.show()\n",
    "img.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/x.tiff')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "forward-month",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1700)\n"
     ]
    }
   ],
   "source": [
    "z = []\n",
    "a = [0,255,0,255,0]\n",
    "for i in range(0,1700):\n",
    "    z.append(a)\n",
    "z = np.array(z,dtype=np.float64,order=\"C\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "img = Image.fromarray(z)\n",
    "print(img.size)\n",
    "img.show()\n",
    "img.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/z.tiff')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "artistic-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1700)\n"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "a = [0,0,255,0,0]\n",
    "for i in range(0,1700):\n",
    "    w.append(a)\n",
    "w = np.array(w,dtype=np.float64,order=\"C\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "img = Image.fromarray(w)\n",
    "print(img.size)\n",
    "img = img.convert('L')\n",
    "img.show()\n",
    "img.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/w.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-reputation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-black",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-analysis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-hometown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-beaver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "adaptive-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 20)\n",
      "[ 351.  839. 1015.  996.  876.  316.  674. 1407.  230.  486.  769. 1386.\n",
      "  157. 1141.   71.   44. 1294.  504.  453.  840.]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1700,5))\n",
    "subset=np.round(np.random.uniform(0,1699,size=20))\n",
    "for i in range(len(subset)):\n",
    "    a[,:] = [1,1,1,1,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "confused-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import sample\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "for x in range(1,101):\n",
    "    a = np.zeros((1700,5))\n",
    "    subset=np.random.uniform(0,1699,size=20)\n",
    "    subset=subset.astype(int)\n",
    "    for i in range(len(subset)):\n",
    "        a[subset[i],:] = [1,1,1,1,1]\n",
    "    a = np.array(a,dtype=np.float64,order=\"C\")\n",
    "    img = Image.fromarray(a)\n",
    "    img = img.convert('L')\n",
    "    img.save('/Volumes/ADATA/random/'+str(x)+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-entity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "overhead-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entrenamiento = '/Volumes/ADATA/random/'\n",
    "data_validacion = '/Volumes/ADATA/random/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "downtown-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas=2\n",
    "longitud,altura = 5,1700\n",
    "\n",
    "batch_size = 5\n",
    "pasos = 1000\n",
    "validation_steps = 300\n",
    "filtrosConv1 = 32\n",
    "filtrosConv2 = 64\n",
    "tamano_filtro1 = (3, 3)\n",
    "tamano_filtro2 = (2, 2)\n",
    "tamano_pool = (2, 2)\n",
    "clases = 1\n",
    "lr = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "united-madagascar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 1 classes.\n",
      "Found 100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "entrenamiento_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validacion_generador = test_datagen.flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "tamil-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(altura, longitud, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(clases, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "floral-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "monthly-advance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10 steps, validate for 10 steps\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 4s 443ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 2s 201ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac8addc490>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(\n",
    "    entrenamiento_generador,\n",
    "    steps_per_epoch=10,\n",
    "    epochs=2,\n",
    "    validation_data=validacion_generador,\n",
    "    validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "revised-violence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/Prueba/random.h5py/assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/Prueba/random.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-motivation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-inspiration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-fruit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-webster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "a = [0,0,255,0,0]\n",
    "for i in range(0,1700):\n",
    "    w.append(a)\n",
    "w = np.array(w,dtype=np.float64,order=\"C\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "img = Image.fromarray(w)\n",
    "print(img.size)\n",
    "img = img.convert('L')\n",
    "img.show()\n",
    "img.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/w.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-directory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-malta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-recorder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "velvet-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "u = []\n",
    "a = ['NaN',2,3,4,5,6,7,8,255,10,11,12,13,14,15,16,255,18,19,20,21,22,23,24,25,0,27,28,29,30,31,200]\n",
    "for i in range(0,32):\n",
    "    u.append(a)\n",
    "u = np.array(u,dtype=np.float64,order=\"C\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from numpy import asarray\n",
    "\n",
    "img = Image.fromarray(u)\n",
    "print(img.size)\n",
    "img = img.convert('RGB')\n",
    "img.show()\n",
    "img.save('/Users/richo/OneDrive - Fundacion Universidad de las Americas Puebla/Honores/CNN/u.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-subsection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-demand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-preserve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-interface",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "latest-provincial",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-5118277fa994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlongitud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maltura\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1700\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodelo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Volumes/ADATA/model.h5py/saved_model.pb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Honores/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "longitud, altura = 5,1700\n",
    "modelo = '/Volumes/ADATA/model.h5py/saved_model.pb'\n",
    "cnn = load_model(modelo)\n",
    "\n",
    "\n",
    "def predict(file):\n",
    "  x = load_img(file, target_size=(longitud, altura))\n",
    "  x = img_to_array(x)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  array = cnn.predict(x)\n",
    "  result = array[0]\n",
    "  answer = np.argmax(result)\n",
    "  if answer == 0:\n",
    "    print(\"pred: Quasar\")\n",
    "  else:\n",
    "    print(\"pred: No Quasar\")\n",
    "\n",
    "  return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
